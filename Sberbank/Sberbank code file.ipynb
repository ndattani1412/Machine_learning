{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Sberbank Russian Housing Market"
      ],
      "metadata": {
        "id": "_JYmW8dTOdp4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Installing necessary libraries"
      ],
      "metadata": {
        "id": "damRma8iOg7J"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x_k4bQtcF8kA",
        "outputId": "64a8c6dc-b0f7-4a59-ce76-4271904c76ab"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting geneticalgorithm\n",
            "  Downloading geneticalgorithm-1.0.2-py3-none-any.whl.metadata (25 kB)\n",
            "Collecting func-timeout (from geneticalgorithm)\n",
            "  Downloading func_timeout-4.3.5.tar.gz (44 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/44.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.3/44.3 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from geneticalgorithm) (1.26.4)\n",
            "Downloading geneticalgorithm-1.0.2-py3-none-any.whl (16 kB)\n",
            "Building wheels for collected packages: func-timeout\n",
            "  Building wheel for func-timeout (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for func-timeout: filename=func_timeout-4.3.5-py3-none-any.whl size=15076 sha256=964264e8c34ed24643dd57e5d3269b3ec9e713cf04b757dd619b4f74d13a19c2\n",
            "  Stored in directory: /root/.cache/pip/wheels/3f/83/19/b5552bb9630e353f7c5b15be44bf10900afe1abbbfcf536afd\n",
            "Successfully built func-timeout\n",
            "Installing collected packages: func-timeout, geneticalgorithm\n",
            "Successfully installed func-timeout-4.3.5 geneticalgorithm-1.0.2\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/dask/dataframe/__init__.py:42: FutureWarning: \n",
            "Dask dataframe query planning is disabled because dask-expr is not installed.\n",
            "\n",
            "You can install it with `pip install dask[dataframe]` or `conda install dask`.\n",
            "This will raise in a future version.\n",
            "\n",
            "  warnings.warn(msg, FutureWarning)\n"
          ]
        }
      ],
      "source": [
        "!pip install geneticalgorithm\n",
        "# Import necessary libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
        "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.linear_model import Ridge\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.base import BaseEstimator, RegressorMixin\n",
        "from sklearn.metrics import mean_squared_log_error\n",
        "import lightgbm as lgb\n",
        "from sklearn.feature_selection import SelectFromModel, SequentialFeatureSelector\n",
        "from geneticalgorithm import geneticalgorithm as ga\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load datasets\n",
        "train = pd.read_csv('/content/train.csv', parse_dates=['timestamp'])\n",
        "test = pd.read_csv('/content/test.csv', parse_dates=['timestamp'])\n",
        "macro = pd.read_csv('/content/macro.csv', parse_dates=['timestamp'])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Merging data"
      ],
      "metadata": {
        "id": "05Af90i9OmvD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-iluTpp_F_Ou"
      },
      "outputs": [],
      "source": [
        "# Merge macroeconomic data\n",
        "macro_cols = [\n",
        "    \"timestamp\", \"balance_trade\", \"balance_trade_growth\", \"eurrub\",\n",
        "    \"average_provision_of_build_contract\", \"micex_rgbi_tr\", \"micex_cbi_tr\",\n",
        "    \"deposits_rate\", \"mortgage_value\", \"mortgage_rate\", \"income_per_cap\",\n",
        "    \"museum_visitis_per_100_cap\", \"apartment_build\"\n",
        "]\n",
        "train = train.merge(macro[macro_cols], on='timestamp', how='left')\n",
        "test = test.merge(macro[macro_cols], on='timestamp', how='left')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Additional features"
      ],
      "metadata": {
        "id": "0y-49xXuOpmZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HvnIpVx2GDhC"
      },
      "outputs": [],
      "source": [
        "# Extract additional features from timestamp before imputation\n",
        "for df in [train, test]:\n",
        "    df['year'] = df['timestamp'].dt.year\n",
        "    df['month'] = df['timestamp'].dt.month\n",
        "    df['day_of_week'] = df['timestamp'].dt.dayofweek\n",
        "# Drop timestamp column after feature extraction\n",
        "train.drop(['timestamp'], axis=1, inplace=True)\n",
        "test.drop(['timestamp'], axis=1, inplace=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Missing values"
      ],
      "metadata": {
        "id": "SoEq0R6MOtWc"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oxNg9W54GEj5"
      },
      "outputs": [],
      "source": [
        "# Handle missing values\n",
        "# Create a list of numerical features, excluding 'price_doc' and 'id' from the test set\n",
        "numerical_features = train.drop(columns=['price_doc', 'id']).select_dtypes(include=np.number).columns.tolist()\n",
        "\n",
        "# Create a SimpleImputer instance with median strategy\n",
        "imputer = SimpleImputer(strategy='median')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Fit and transform"
      ],
      "metadata": {
        "id": "wLfckYTDOvxd"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5ukunWS-GHl8"
      },
      "outputs": [],
      "source": [
        "# Fit and transform the imputer on numerical features only\n",
        "train[numerical_features] = imputer.fit_transform(train[numerical_features])\n",
        "test[numerical_features] = imputer.transform(test[numerical_features])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GlpqmZEOGKP-"
      },
      "outputs": [],
      "source": [
        "# Log-transform the target variable\n",
        "y = np.log1p(train['price_doc'])\n",
        "X = train.drop(['id', 'price_doc'], axis=1)\n",
        "test_ids = test['id']\n",
        "X_test = test.drop(['id'], axis=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Non numeric features"
      ],
      "metadata": {
        "id": "jKbUTRy4O4gV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uEUbAYHAGMXU"
      },
      "outputs": [],
      "source": [
        "# Handle non-numeric features (e.g., 'product_type')\n",
        "\n",
        "# Create a LabelEncoder instance\n",
        "encoder = LabelEncoder()\n",
        "\n",
        "# Combine 'product_type' from both train and test data for fitting\n",
        "all_product_types = pd.concat([X['product_type'], X_test['product_type']], ignore_index=True)\n",
        "\n",
        "# Convert all_product_types to string type to handle mixed types and NaNs\n",
        "all_product_types = all_product_types.astype(str)\n",
        "\n",
        "# Fit the encoder on the combined data\n",
        "encoder.fit(all_product_types)\n",
        "\n",
        "# Transform 'product_type' in both train and test data\n",
        "X['product_type'] = encoder.transform(X['product_type'].astype(str)) # Ensure consistent type during transform\n",
        "X_test['product_type'] = encoder.transform(X_test['product_type'].astype(str)) # Ensure consistent type during transform"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Conversion"
      ],
      "metadata": {
        "id": "lfdwG6AZO2Ln"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gtkwzf75GPlV"
      },
      "outputs": [],
      "source": [
        "# Handle non-numeric features\n",
        "# Convert all object (string) type columns to numerical using Label Encoding\n",
        "for col in X.select_dtypes(include=['object']).columns:\n",
        "    # Create a LabelEncoder instance\n",
        "    encoder = LabelEncoder()\n",
        "\n",
        "    # Combine data from both train and test for fitting\n",
        "    all_data = pd.concat([X[col], X_test[col]], ignore_index=True)\n",
        "\n",
        "    # Convert to string type to handle mixed types and NaNs\n",
        "    all_data = all_data.astype(str)\n",
        "\n",
        "    # Fit the encoder on the combined data\n",
        "    encoder.fit(all_data)\n",
        "\n",
        "    # Transform the column in both train and test data\n",
        "    X[col] = encoder.transform(X[col].astype(str))\n",
        "    X_test[col] = encoder.transform(X_test[col].astype(str))\n",
        "\n",
        "# Now you can scale the data:\n",
        "scaler = StandardScaler()\n",
        "X = pd.DataFrame(scaler.fit_transform(X), columns=X.columns)\n",
        "X_test = pd.DataFrame(scaler.transform(X_test), columns=X_test.columns)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Model evaluation- define function"
      ],
      "metadata": {
        "id": "4C9IlppgO7cG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JAV-BCAEGRoH"
      },
      "outputs": [],
      "source": [
        "# Define function to evaluate model performance\n",
        "def rmsle(y_true, y_pred):\n",
        "    return np.sqrt(mean_squared_log_error(y_true, y_pred))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cross validation"
      ],
      "metadata": {
        "id": "PQWfbwVdO_Jm"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hI-8rslKGT2D"
      },
      "outputs": [],
      "source": [
        "# Split data for cross-validation\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define models"
      ],
      "metadata": {
        "id": "cHOOwVwZPEhm"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "awj89OHgGWAN"
      },
      "outputs": [],
      "source": [
        "# Define models for experimentation\n",
        "models = {\n",
        "    'Random Forest': RandomForestRegressor(random_state=42),\n",
        "    'Gradient Boosting': GradientBoostingRegressor(random_state=42),\n",
        "    'Support Vector Machines': SVR(),\n",
        "    'LightGBM': lgb.LGBMRegressor(random_state=42)\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hyperparameter tuning"
      ],
      "metadata": {
        "id": "xGefH0gSPB3X"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "ebePK67hGYcY",
        "outputId": "0ad8acf9-34c9-4a09-ba33-01314aeeb7af"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tuning Random Forest...\n",
            "Best Random Forest Model: {'max_depth': 10, 'n_estimators': 200}\n",
            "Tuning Gradient Boosting...\n",
            "Best Gradient Boosting Model: {'learning_rate': 0.1, 'n_estimators': 200}\n",
            "Tuning Support Vector Machines...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_search.py:1103: UserWarning: One or more of the test scores are non-finite: [-0.00119409 -0.00095309         nan -0.00091443         nan -0.00092913]\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best Support Vector Machines Model: {'C': 1, 'kernel': 'rbf'}\n",
            "Tuning LightGBM...\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.018022 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 42245\n",
            "[LightGBM] [Info] Number of data points in the train set: 24376, number of used features: 304\n",
            "[LightGBM] [Info] Start training from score 15.612234\n",
            "Best LightGBM Model: {'learning_rate': 0.1, 'n_estimators': 100}\n"
          ]
        }
      ],
      "source": [
        "# Hyperparameter tuning and model evaluation\n",
        "best_models = {}\n",
        "for name, model in models.items():\n",
        "    print(f\"Tuning {name}...\")\n",
        "    if name == 'Random Forest':\n",
        "        params = {'n_estimators': [100, 200], 'max_depth': [10, 20]}\n",
        "    elif name == 'Gradient Boosting':\n",
        "        params = {'learning_rate': [0.01, 0.1], 'n_estimators': [100, 200]}\n",
        "    elif name == 'Support Vector Machines':\n",
        "        params = {'C': [0.1, 1, 10], 'kernel': ['linear', 'rbf']}\n",
        "    elif name == 'LightGBM':\n",
        "        params = {'learning_rate': [0.01, 0.1], 'n_estimators': [100, 200]}\n",
        "\n",
        "    grid = GridSearchCV(model, params, cv=3, scoring='neg_mean_squared_log_error', n_jobs=-1)\n",
        "    grid.fit(X_train, y_train)\n",
        "    best_models[name] = grid.best_estimator_\n",
        "    print(f\"Best {name} Model: {grid.best_params_}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Evaluate models"
      ],
      "metadata": {
        "id": "fVqgkjKHPIJo"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "7zzRlrAwGal7",
        "outputId": "c256485e-82db-489a-eb7b-c2f83f880b44"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Random Forest RMSLE: 12.799684451183952\n",
            "Gradient Boosting RMSLE: 12.797154617075973\n",
            "Support Vector Machines RMSLE: 12.901403773164619\n",
            "LightGBM RMSLE: 12.803517644238045\n"
          ]
        }
      ],
      "source": [
        "# Evaluate models on validation data\n",
        "for name, model in best_models.items():\n",
        "    y_pred = model.predict(X_val)\n",
        "    print(f\"{name} RMSLE: {rmsle(y_val, np.expm1(y_pred))}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Stack models"
      ],
      "metadata": {
        "id": "cEl0klvcPKNs"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xTj15E4YGcqm",
        "outputId": "fdd35674-fc28-4cc5-d132-46ca3f472d16"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.018325 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 42245\n",
            "[LightGBM] [Info] Number of data points in the train set: 24376, number of used features: 304\n",
            "[LightGBM] [Info] Start training from score 15.612234\n",
            "Stacked Model RMSLE: 12.800290700809185\n"
          ]
        }
      ],
      "source": [
        "# Stacking models\n",
        "class StackingModel(BaseEstimator, RegressorMixin):\n",
        "    def __init__(self, base_models, meta_model):\n",
        "        self.base_models = base_models\n",
        "        self.meta_model = meta_model\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        self.base_models_ = [model.fit(X, y) for model in self.base_models]\n",
        "        meta_features = np.column_stack([model.predict(X) for model in self.base_models_])\n",
        "        self.meta_model_ = self.meta_model.fit(meta_features, y)\n",
        "        return self\n",
        "\n",
        "    def predict(self, X):\n",
        "        meta_features = np.column_stack([model.predict(X) for model in self.base_models_])\n",
        "        return self.meta_model_.predict(meta_features)\n",
        "\n",
        "stacked_model = StackingModel(\n",
        "    base_models=list(best_models.values()),\n",
        "    meta_model=Ridge()\n",
        ")\n",
        "stacked_model.fit(X_train, y_train)\n",
        "stacked_pred = stacked_model.predict(X_val)\n",
        "print(f\"Stacked Model RMSLE: {rmsle(y_val, np.expm1(stacked_pred))}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Kaggle submission"
      ],
      "metadata": {
        "id": "IrpwwyaXPOLP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate Kaggle submission\n",
        "stacked_test_pred = np.expm1(stacked_model.predict(X_test))\n",
        "submission = pd.DataFrame({'id': test_ids, 'price_doc': stacked_test_pred})\n",
        "submission.to_csv('stacked_submission.csv', index=False)"
      ],
      "metadata": {
        "id": "LVQZm1f4r2gy"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}